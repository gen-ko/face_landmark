{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from face_landmark import model\n",
    "from face_landmark import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "dump_dir = '/barn2/yuan/datasets/300W_LP_mini/json'\n",
    "data_iterator = load_dataset.DatasetIteratorWithHeatmap(dump_dir=dump_dir)\n",
    "print(len(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61225\n"
     ]
    }
   ],
   "source": [
    "dump_dir = '/barn2/yuan/datasets/300W_LP/json'\n",
    "data_iterator = load_dataset.DatasetIteratorWithHeatmap(dump_dir=dump_dir)\n",
    "print(len(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del sess0\n",
    "    del sess\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del graph0\n",
    "    del session\n",
    "except:\n",
    "    pass\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.4368111226293776\n",
      "epoch: 1 loss: 1.1972072124481201\n",
      "epoch: 2 loss: 1.0517540739642248\n",
      "epoch: 3 loss: 0.9463422927591536\n",
      "epoch: 4 loss: 0.8636044992340935\n",
      "epoch: 5 loss: 0.7953388161129422\n",
      "epoch: 6 loss: 0.7372961507903205\n",
      "epoch: 7 loss: 0.6870713068379296\n",
      "epoch: 8 loss: 0.6428966853353713\n",
      "epoch: 9 loss: 0.6035722196102142\n",
      "epoch: 10 loss: 0.5681571596198611\n",
      "epoch: 11 loss: 0.5359877049922943\n",
      "epoch: 12 loss: 0.5066475023825964\n",
      "epoch: 13 loss: 0.47977151970068616\n",
      "epoch: 14 loss: 0.4550356964270274\n",
      "epoch: 15 loss: 0.4321424000793033\n",
      "epoch: 16 loss: 0.4108474552631378\n",
      "epoch: 17 loss: 0.39097684456242454\n",
      "epoch: 18 loss: 0.3723641468418969\n",
      "epoch: 19 loss: 0.35488467249605393\n",
      "epoch: 20 loss: 0.3384358170959685\n",
      "epoch: 21 loss: 0.32292132410738206\n",
      "epoch: 22 loss: 0.3082752111885283\n",
      "epoch: 23 loss: 0.29442427721288467\n",
      "epoch: 24 loss: 0.281302230225669\n",
      "epoch: 25 loss: 0.2688538233439128\n",
      "epoch: 26 loss: 0.25703509400288266\n",
      "epoch: 27 loss: 0.2457987442612648\n",
      "epoch: 28 loss: 0.23510718759563234\n",
      "epoch: 29 loss: 0.22492983274989659\n",
      "epoch: 30 loss: 0.2152355263630549\n",
      "epoch: 31 loss: 0.20599690824747086\n",
      "epoch: 32 loss: 0.19718824244207805\n",
      "epoch: 33 loss: 0.18878574834929573\n",
      "epoch: 34 loss: 0.180769521329138\n",
      "epoch: 35 loss: 0.17311888602044848\n",
      "epoch: 36 loss: 0.1658133872681194\n",
      "epoch: 37 loss: 0.15883859826458824\n",
      "epoch: 38 loss: 0.152177256014612\n",
      "epoch: 39 loss: 0.1458129965596729\n",
      "epoch: 40 loss: 0.13973064141141045\n",
      "epoch: 41 loss: 0.13391872495412827\n",
      "epoch: 42 loss: 0.128366622245974\n",
      "epoch: 43 loss: 0.12306062959962422\n",
      "epoch: 44 loss: 0.11798918288615015\n",
      "epoch: 45 loss: 0.11314346144596736\n",
      "epoch: 46 loss: 0.10851240944531229\n",
      "epoch: 47 loss: 0.10408594376511043\n",
      "epoch: 48 loss: 0.09985512329472436\n",
      "epoch: 49 loss: 0.09581190720200539\n",
      "epoch: 50 loss: 0.09194656254516707\n",
      "epoch: 51 loss: 0.08825218967265552\n",
      "epoch: 52 loss: 0.0847215751806895\n",
      "epoch: 53 loss: 0.0813473905954096\n",
      "epoch: 54 loss: 0.07812172795335452\n",
      "epoch: 55 loss: 0.07503895751304096\n",
      "epoch: 56 loss: 0.07209348719980982\n",
      "epoch: 57 loss: 0.06927958586149746\n",
      "epoch: 58 loss: 0.06659154904385407\n",
      "epoch: 59 loss: 0.06402290612459183\n",
      "epoch: 60 loss: 0.06156800645920965\n",
      "epoch: 61 loss: 0.05922171576983399\n",
      "epoch: 62 loss: 0.05697987352808317\n",
      "epoch: 63 loss: 0.05483764472107092\n",
      "epoch: 64 loss: 0.052790860128071576\n",
      "epoch: 65 loss: 0.05083551878730456\n",
      "epoch: 66 loss: 0.04896822551058398\n",
      "epoch: 67 loss: 0.04718453147345119\n",
      "epoch: 68 loss: 0.04548071759442488\n",
      "epoch: 69 loss: 0.043853465053770274\n",
      "epoch: 70 loss: 0.0422990077899562\n",
      "epoch: 71 loss: 0.04081479646265507\n",
      "epoch: 72 loss: 0.039397450577881604\n",
      "epoch: 73 loss: 0.038044076412916183\n",
      "epoch: 74 loss: 0.036751345420877136\n",
      "epoch: 75 loss: 0.03551687652038203\n",
      "epoch: 76 loss: 0.034337607212364674\n",
      "epoch: 77 loss: 0.03321170651664337\n",
      "epoch: 78 loss: 0.032136561142073736\n",
      "epoch: 79 loss: 0.031109459180798795\n",
      "epoch: 80 loss: 0.030128568100432556\n",
      "epoch: 81 loss: 0.02919187324328555\n",
      "epoch: 82 loss: 0.028297050649093256\n",
      "epoch: 83 loss: 0.02744262344721291\n",
      "epoch: 84 loss: 0.0266267711089717\n",
      "epoch: 85 loss: 0.025847136249972716\n",
      "epoch: 86 loss: 0.025102469242281385\n",
      "epoch: 87 loss: 0.02439147834148672\n",
      "epoch: 88 loss: 0.023712017056014802\n",
      "epoch: 89 loss: 0.02306284610595968\n",
      "epoch: 90 loss: 0.022442572543190584\n",
      "epoch: 91 loss: 0.02184960266782178\n",
      "epoch: 92 loss: 0.02128294472479158\n",
      "epoch: 93 loss: 0.020741008532543976\n",
      "epoch: 94 loss: 0.02022276870492432\n",
      "epoch: 95 loss: 0.019727044842309423\n",
      "epoch: 96 loss: 0.01925276168104675\n",
      "epoch: 97 loss: 0.018798745444251433\n",
      "epoch: 98 loss: 0.018363955844607618\n",
      "epoch: 99 loss: 0.017947833726389542\n",
      "epoch: 100 loss: 0.01754902820620272\n",
      "epoch: 101 loss: 0.017166957155697875\n",
      "epoch: 102 loss: 0.016800620386170015\n",
      "epoch: 103 loss: 0.016449604959537584\n",
      "epoch: 104 loss: 0.01611263372210993\n",
      "epoch: 105 loss: 0.01578940808152159\n",
      "epoch: 106 loss: 0.015479157834003368\n",
      "epoch: 107 loss: 0.015181215790410837\n",
      "epoch: 108 loss: 0.01489524051754011\n",
      "epoch: 109 loss: 0.014620401430875063\n",
      "epoch: 110 loss: 0.014356146204388805\n",
      "epoch: 111 loss: 0.01410198470370637\n",
      "epoch: 112 loss: 0.013857654709782865\n",
      "epoch: 113 loss: 0.013622402782655425\n",
      "epoch: 114 loss: 0.01339576972855462\n",
      "epoch: 115 loss: 0.01317730892656578\n",
      "epoch: 116 loss: 0.01296662947990828\n",
      "epoch: 117 loss: 0.012763294960475631\n",
      "epoch: 118 loss: 0.01256702421233058\n",
      "epoch: 119 loss: 0.012377335845182339\n",
      "epoch: 120 loss: 0.012194130207515426\n",
      "epoch: 121 loss: 0.01201692254592975\n",
      "epoch: 122 loss: 0.011845567315402959\n",
      "epoch: 123 loss: 0.011679611324022213\n",
      "epoch: 124 loss: 0.011519007333036926\n",
      "epoch: 125 loss: 0.011363239182780186\n",
      "epoch: 126 loss: 0.011212290005965365\n",
      "epoch: 127 loss: 0.011065813195374277\n",
      "epoch: 128 loss: 0.010923504622446166\n",
      "epoch: 129 loss: 0.010785424564447667\n",
      "epoch: 130 loss: 0.010651045478880405\n",
      "epoch: 131 loss: 0.010520518084781038\n",
      "epoch: 132 loss: 0.010393328757749664\n",
      "epoch: 133 loss: 0.010269697910795609\n",
      "epoch: 134 loss: 0.010149248151315583\n",
      "epoch: 135 loss: 0.010031927997867266\n",
      "epoch: 136 loss: 0.009917458519339561\n",
      "epoch: 137 loss: 0.009805985105534395\n",
      "epoch: 138 loss: 0.009697006342725622\n",
      "epoch: 139 loss: 0.00959064651073681\n",
      "epoch: 140 loss: 0.009486730365703503\n",
      "epoch: 141 loss: 0.009385239953796068\n",
      "epoch: 142 loss: 0.009286034205514524\n",
      "epoch: 143 loss: 0.009188915499382548\n",
      "epoch: 144 loss: 0.009093955935289463\n",
      "epoch: 145 loss: 0.00900110273828937\n",
      "epoch: 146 loss: 0.008910048701283004\n",
      "epoch: 147 loss: 0.008820838605364164\n",
      "epoch: 148 loss: 0.00873334377279712\n",
      "epoch: 149 loss: 0.008647522526896663\n",
      "epoch: 150 loss: 0.008563310218354067\n",
      "epoch: 151 loss: 0.008480764382208386\n",
      "epoch: 152 loss: 0.00839960696693096\n",
      "epoch: 153 loss: 0.00832001632079482\n",
      "epoch: 154 loss: 0.008241840353649523\n",
      "epoch: 155 loss: 0.008165030041709542\n",
      "epoch: 156 loss: 0.008089460639490021\n",
      "epoch: 157 loss: 0.008015380163366595\n",
      "epoch: 158 loss: 0.007942479233153991\n",
      "epoch: 159 loss: 0.007870787211383382\n",
      "epoch: 160 loss: 0.0078003555277569425\n",
      "epoch: 161 loss: 0.0077310067135840654\n",
      "epoch: 162 loss: 0.007662845180473394\n",
      "epoch: 163 loss: 0.007595682439083855\n",
      "epoch: 164 loss: 0.007529601196034087\n",
      "epoch: 165 loss: 0.007464592422669132\n",
      "epoch: 166 loss: 0.007400430402615004\n",
      "epoch: 167 loss: 0.007337286240524716\n",
      "epoch: 168 loss: 0.007275085741033156\n",
      "epoch: 169 loss: 0.00721377439589964\n",
      "epoch: 170 loss: 0.007153350290738874\n",
      "epoch: 171 loss: 0.007093832802234424\n",
      "epoch: 172 loss: 0.007035045781069332\n",
      "epoch: 173 loss: 0.0069772131327125765\n",
      "epoch: 174 loss: 0.006920077087771561\n",
      "epoch: 175 loss: 0.006863844167027209\n",
      "epoch: 176 loss: 0.006808307642738025\n",
      "epoch: 177 loss: 0.006753574203078945\n",
      "epoch: 178 loss: 0.006699592366607653\n",
      "epoch: 179 loss: 0.006646280978909797\n",
      "epoch: 180 loss: 0.00659358498847319\n",
      "epoch: 181 loss: 0.006541584980570608\n",
      "epoch: 182 loss: 0.006490245952995287\n",
      "epoch: 183 loss: 0.0064395634302248555\n",
      "epoch: 184 loss: 0.006389501116548975\n",
      "epoch: 185 loss: 0.006340113804779119\n",
      "epoch: 186 loss: 0.006291258329939511\n",
      "epoch: 187 loss: 0.006243104994710948\n",
      "epoch: 188 loss: 0.006195495060334603\n",
      "epoch: 189 loss: 0.006148473075073626\n",
      "epoch: 190 loss: 0.006101993197161291\n",
      "epoch: 191 loss: 0.006056070250148575\n",
      "epoch: 192 loss: 0.006010719704338246\n",
      "epoch: 193 loss: 0.005965823359373543\n",
      "epoch: 194 loss: 0.005921557856102784\n",
      "epoch: 195 loss: 0.005877745182563861\n",
      "epoch: 196 loss: 0.005834439588296745\n",
      "epoch: 197 loss: 0.005791544448584318\n",
      "epoch: 198 loss: 0.0057493333653029464\n",
      "epoch: 199 loss: 0.0057074563422550755\n"
     ]
    }
   ],
   "source": [
    "graph0 = tf.Graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(target='', graph=graph0, config=config) as sess:\n",
    "    image_tensor = tf.placeholder(dtype=tf.uint8, shape=[None, None, None, 3])\n",
    "    x1 = (tf.cast(x=image_tensor, dtype=tf.float32) * 2.0 / 255.0) - 1.0\n",
    "    x2 = tf.image.resize_images(x1, size=(256, 256))\n",
    "    heatmap_inferred_tensors = model.fan(x=x2, num_modules=1)\n",
    "    heatmap_inferred_t1_tensor = heatmap_inferred_tensors[0]\n",
    "    heatmap_groundtruth_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 64, 64, 68])\n",
    "    loss_op = tf.losses.mean_squared_error(labels=heatmap_groundtruth_tensor, predictions=heatmap_inferred_t1_tensor)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss_op)\n",
    "    \n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    n_epoch = 200\n",
    "    n_steps = len(data_iterator)\n",
    "\n",
    "    for i_epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        for i_step, (image_sample, heatmap_sample) in enumerate(data_iterator):\n",
    "            loss_i, _ = sess.run([loss_op, train_op], \n",
    "                                  feed_dict={image_tensor:[image_sample],\n",
    "                                             heatmap_groundtruth_tensor:[heatmap_sample]})\n",
    "            loss += loss_i\n",
    "        print('epoch:', i_epoch, 'loss:', loss / n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 64, 64, 68), dtype=float32)\n",
      "in_channels: 64\n",
      "out_channels: 128\n",
      "in_channels: 128\n",
      "out_channels: 256\n",
      "previous shape: (?, 64, 64, 256)\n",
      "low1 shape: (?, 32, 32, 256)\n",
      "low1 shape: (?, 16, 16, 256)\n",
      "low1 shape: (?, 8, 8, 256)\n",
      "low1 shape: (?, 4, 4, 256)\n",
      "previous shape: (?, 64, 64, 256)\n",
      "low1 shape: (?, 32, 32, 256)\n",
      "low1 shape: (?, 16, 16, 256)\n",
      "low1 shape: (?, 8, 8, 256)\n",
      "low1 shape: (?, 4, 4, 256)\n",
      "previous shape: (?, 64, 64, 256)\n",
      "low1 shape: (?, 32, 32, 256)\n",
      "low1 shape: (?, 16, 16, 256)\n",
      "low1 shape: (?, 8, 8, 256)\n",
      "low1 shape: (?, 4, 4, 256)\n",
      "previous shape: (?, 64, 64, 256)\n",
      "low1 shape: (?, 32, 32, 256)\n",
      "low1 shape: (?, 16, 16, 256)\n",
      "low1 shape: (?, 8, 8, 256)\n",
      "low1 shape: (?, 4, 4, 256)\n",
      "inferred length:  4\n",
      "epoch: 1 loss: 5.876730585098267\n",
      "epoch: 1 loss: 5.567057192325592\n",
      "epoch: 1 loss: 5.304811803499858\n",
      "epoch: 1 loss: 5.074772834777832\n",
      "epoch: 1 loss: 4.863504695892334\n",
      "epoch: 1 loss: 4.671355456113815\n",
      "epoch: 1 loss: 4.500126474244254\n",
      "epoch: 1 loss: 4.344352230429649\n",
      "epoch: 1 loss: 4.201813867357042\n",
      "epoch: 1 loss: 4.068515440225601\n",
      "epoch: 1 loss: 3.9461735844612122\n",
      "epoch: 1 loss: 3.8323345134655633\n",
      "epoch: 1 loss: 3.727227354966677\n",
      "epoch: 1 loss: 3.62607884151595\n",
      "epoch: 1 loss: 3.531749196052551\n",
      "epoch: 1 loss: 3.4431496158242227\n",
      "epoch: 1 loss: 3.3604349336203407\n",
      "epoch: 1 loss: 3.282482184966405\n",
      "epoch: 1 loss: 3.208978289679477\n",
      "epoch: 1 loss: 3.1383007892966273\n",
      "epoch: 1 loss: 3.071972794759841\n",
      "epoch: 1 loss: 3.0086980044841765\n",
      "epoch: 1 loss: 2.9487410190312757\n",
      "epoch: 1 loss: 2.8905601419508455\n",
      "epoch: 1 loss: 2.8347781510353087\n",
      "epoch: 1 loss: 2.782282633506335\n",
      "epoch: 1 loss: 2.7314550514574405\n",
      "epoch: 1 loss: 2.6821438576493946\n",
      "epoch: 1 loss: 2.635028504297651\n",
      "epoch: 1 loss: 2.589982348481814\n",
      "epoch: 1 loss: 2.546344937239924\n",
      "epoch: 1 loss: 2.5040146129205825\n",
      "epoch: 1 loss: 2.4635058171821362\n",
      "epoch: 1 loss: 2.424394660136279\n",
      "epoch: 1 loss: 2.3868529624598365\n",
      "epoch: 1 loss: 2.3505813298953906\n",
      "epoch: 1 loss: 2.3154294396574433\n",
      "epoch: 1 loss: 2.2812341215579135\n",
      "epoch: 1 loss: 2.2477595889415496\n",
      "epoch: 1 loss: 2.215497921258211\n",
      "epoch: 1 loss: 2.1839333025420586\n",
      "epoch: 1 loss: 2.1535295863946278\n",
      "epoch: 1 loss: 2.1241007231695708\n",
      "epoch: 1 loss: 2.0958704602989284\n",
      "epoch: 1 loss: 2.068193291227023\n",
      "epoch: 1 loss: 2.041302573292152\n",
      "epoch: 1 loss: 2.015053058811959\n",
      "epoch: 1 loss: 1.9895241328825553\n",
      "epoch: 1 loss: 1.964723234821339\n",
      "epoch: 1 loss: 1.9406537581086158\n",
      "epoch: 1 loss: 1.9169834084370556\n",
      "epoch: 1 loss: 1.8939895218954637\n",
      "epoch: 1 loss: 1.871496436337255\n",
      "epoch: 1 loss: 1.8497448207051665\n",
      "epoch: 1 loss: 1.8284397880055687\n",
      "epoch: 1 loss: 1.8077700715512037\n",
      "epoch: 1 loss: 1.7875179496773503\n",
      "epoch: 1 loss: 1.7678756958965598\n",
      "epoch: 1 loss: 1.7482644170017565\n",
      "epoch: 1 loss: 1.7293297680219015\n",
      "epoch: 1 loss: 1.7110945781723397\n",
      "epoch: 1 loss: 1.6929160476211578\n",
      "epoch: 1 loss: 1.6752988189931899\n",
      "epoch: 1 loss: 1.6581524862907826\n",
      "epoch: 1 loss: 1.6412893411746392\n",
      "epoch: 1 loss: 1.6248012006282806\n",
      "epoch: 1 loss: 1.608656631988376\n",
      "epoch: 1 loss: 1.5928455505081835\n",
      "epoch: 1 loss: 1.5773899302534435\n",
      "epoch: 1 loss: 1.5622848702754293\n",
      "epoch: 1 loss: 1.5473890943846234\n",
      "epoch: 1 loss: 1.5327623926723997\n",
      "epoch: 1 loss: 1.5187267875630561\n",
      "epoch: 1 loss: 1.5049519711249584\n",
      "epoch: 1 loss: 1.4914660064180691\n",
      "epoch: 1 loss: 1.4780126469896029\n",
      "epoch: 1 loss: 1.464962076269961\n",
      "epoch: 1 loss: 1.4520245236846117\n",
      "epoch: 1 loss: 1.4395488092039206\n",
      "epoch: 1 loss: 1.4273885085247457\n",
      "epoch: 1 loss: 1.4151914383342237\n",
      "epoch: 1 loss: 1.4034570448100567\n",
      "epoch: 1 loss: 1.3918456075780363\n",
      "epoch: 1 loss: 1.3803408410222757\n",
      "epoch: 1 loss: 1.369354575416621\n",
      "epoch: 1 loss: 1.358288302341866\n",
      "epoch: 1 loss: 1.347546736519227\n",
      "epoch: 1 loss: 1.336906791681593\n",
      "epoch: 1 loss: 1.32664593041278\n",
      "epoch: 1 loss: 1.3164068616429965\n",
      "epoch: 1 loss: 1.3063370047198548\n",
      "epoch: 1 loss: 1.2965552494901678\n",
      "epoch: 1 loss: 1.2868973578336418\n",
      "epoch: 1 loss: 1.277560223781682\n",
      "epoch: 1 loss: 1.268376515068506\n",
      "epoch: 1 loss: 1.259251835414519\n",
      "epoch: 1 loss: 1.2502273397500983\n",
      "epoch: 1 loss: 1.2412847809797647\n",
      "epoch: 1 loss: 1.232631945113341\n",
      "epoch: 1 loss: 1.2240325180888176\n",
      "epoch: 1 loss: 1.2157038455847466\n",
      "epoch: 1 loss: 1.2075378114394113\n",
      "epoch: 1 loss: 1.1994732336077876\n",
      "epoch: 1 loss: 1.1915125041340406\n",
      "epoch: 1 loss: 1.1837101717648053\n",
      "epoch: 1 loss: 1.1759214254904469\n",
      "epoch: 1 loss: 1.1682590929007977\n",
      "epoch: 1 loss: 1.1608015680064758\n",
      "epoch: 1 loss: 1.1534392827831277\n",
      "epoch: 1 loss: 1.146075783602216\n",
      "epoch: 1 loss: 1.1388965284233694\n",
      "epoch: 1 loss: 1.1319813789001534\n",
      "epoch: 1 loss: 1.1250308512850145\n",
      "epoch: 1 loss: 1.1181415129387589\n",
      "epoch: 1 loss: 1.1115157768130302\n",
      "epoch: 1 loss: 1.1048338639582025\n",
      "epoch: 1 loss: 1.0983282319373555\n",
      "epoch: 1 loss: 1.0918105057993177\n",
      "epoch: 1 loss: 1.0853864208615127\n",
      "epoch: 1 loss: 1.0790296930943926\n",
      "epoch: 1 loss: 1.0727478452704169\n",
      "epoch: 1 loss: 1.0666459161238593\n",
      "epoch: 1 loss: 1.060667961642025\n",
      "epoch: 1 loss: 1.054748465937953\n",
      "epoch: 1 loss: 1.0489008706569671\n",
      "epoch: 1 loss: 1.0431741339228455\n",
      "epoch: 1 loss: 1.037521776991097\n",
      "epoch: 1 loss: 1.0319836496841162\n",
      "epoch: 1 loss: 1.026457997163137\n",
      "epoch: 1 loss: 1.0209941170192682\n",
      "epoch: 1 loss: 1.0156487757126793\n",
      "epoch: 1 loss: 1.010450415448709\n",
      "epoch: 1 loss: 1.0052075170036545\n",
      "epoch: 1 loss: 1.00010742427253\n",
      "epoch: 1 loss: 0.9950027681942339\n",
      "epoch: 1 loss: 0.9899426706792677\n",
      "epoch: 1 loss: 0.9849487878654125\n",
      "epoch: 1 loss: 0.9799947630830002\n",
      "epoch: 1 loss: 0.9751035517657832\n",
      "epoch: 1 loss: 0.9702639474666545\n",
      "epoch: 1 loss: 0.9655592514003845\n",
      "epoch: 1 loss: 0.960935597336838\n",
      "epoch: 1 loss: 0.9563599229536274\n",
      "epoch: 1 loss: 0.9518222031649202\n",
      "epoch: 1 loss: 0.9473652640653067\n",
      "epoch: 1 loss: 0.9428826131896205\n",
      "epoch: 1 loss: 0.9384401830635509\n",
      "epoch: 1 loss: 0.934112180583179\n",
      "epoch: 1 loss: 0.9298651325422645\n",
      "epoch: 1 loss: 0.9257256542146206\n",
      "epoch: 1 loss: 0.9215102332317276\n",
      "epoch: 1 loss: 0.9173073197028747\n",
      "epoch: 1 loss: 0.913148799543482\n",
      "epoch: 1 loss: 0.90900090382761\n",
      "epoch: 1 loss: 0.904995300933238\n",
      "epoch: 1 loss: 0.9010656920572122\n",
      "epoch: 1 loss: 0.8971260306087269\n",
      "epoch: 1 loss: 0.8932782367909257\n",
      "epoch: 1 loss: 0.8894628111765069\n",
      "epoch: 1 loss: 0.8856310448562726\n",
      "epoch: 1 loss: 0.8818644224162798\n",
      "epoch: 1 loss: 0.8781542817383636\n",
      "epoch: 1 loss: 0.8744385242690703\n",
      "epoch: 1 loss: 0.8707954327007983\n",
      "epoch: 1 loss: 0.8672678408839486\n",
      "epoch: 1 loss: 0.8637763985443905\n",
      "epoch: 1 loss: 0.8603570954394555\n",
      "epoch: 1 loss: 0.8568908923482966\n",
      "epoch: 1 loss: 0.8534034067401166\n",
      "epoch: 1 loss: 0.8500598677773686\n",
      "epoch: 1 loss: 0.8466409119989788\n",
      "epoch: 1 loss: 0.8433141824767687\n",
      "epoch: 1 loss: 0.8399993952815933\n",
      "epoch: 1 loss: 0.8367022148322785\n",
      "epoch: 1 loss: 0.833469589731523\n",
      "epoch: 1 loss: 0.8302736945534972\n",
      "epoch: 1 loss: 0.8270927420948858\n",
      "epoch: 1 loss: 0.8239411095429338\n",
      "epoch: 1 loss: 0.8208044256702814\n",
      "epoch: 1 loss: 0.8176847717455692\n",
      "epoch: 1 loss: 0.8147020762755397\n",
      "epoch: 1 loss: 0.8116004868564043\n",
      "epoch: 1 loss: 0.8085365860595729\n",
      "epoch: 1 loss: 0.8055558238340461\n",
      "epoch: 1 loss: 0.8025959496804186\n",
      "epoch: 1 loss: 0.799679641754076\n",
      "epoch: 1 loss: 0.7968998409847524\n",
      "epoch: 1 loss: 0.7940652894648783\n",
      "epoch: 1 loss: 0.7913486548083485\n",
      "epoch: 1 loss: 0.7885809738463477\n",
      "epoch: 1 loss: 0.7857463593297291\n",
      "epoch: 1 loss: 0.7847623743266763\n",
      "epoch: 2 loss: 0.2929870508611202\n",
      "epoch: 2 loss: 0.29148094579577444\n",
      "epoch: 2 loss: 0.2861860734721025\n",
      "epoch: 2 loss: 0.2844507431611419\n",
      "epoch: 2 loss: 0.28153268799185754\n",
      "epoch: 2 loss: 0.2785000545283159\n",
      "epoch: 2 loss: 0.2777863926121167\n",
      "epoch: 2 loss: 0.27674988890066743\n",
      "epoch: 2 loss: 0.27519262863530053\n",
      "epoch: 2 loss: 0.2749485431611538\n",
      "epoch: 2 loss: 0.273942831158638\n",
      "epoch: 2 loss: 0.27389775638779007\n",
      "epoch: 2 loss: 0.27325095110214676\n",
      "epoch: 2 loss: 0.2716576930667673\n",
      "epoch: 2 loss: 0.27068555369973185\n",
      "epoch: 2 loss: 0.2702751580160111\n",
      "epoch: 2 loss: 0.26968605430687176\n",
      "epoch: 2 loss: 0.2694448520325952\n",
      "epoch: 2 loss: 0.2689303911045978\n",
      "epoch: 2 loss: 0.26852155581116677\n",
      "epoch: 2 loss: 0.2681406314529124\n",
      "epoch: 2 loss: 0.2674044758758762\n",
      "epoch: 2 loss: 0.2667201294523218\n",
      "epoch: 2 loss: 0.2657027380230526\n",
      "epoch: 2 loss: 0.2652188270092011\n",
      "epoch: 2 loss: 0.26480599395357646\n",
      "epoch: 2 loss: 0.26512045347028307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.26487569542867795\n",
      "epoch: 2 loss: 0.2646452578491178\n",
      "epoch: 2 loss: 0.2642681780954202\n",
      "epoch: 2 loss: 0.2638268022527618\n",
      "epoch: 2 loss: 0.26303189613390715\n",
      "epoch: 2 loss: 0.26258318211996196\n",
      "epoch: 2 loss: 0.2621496223351535\n",
      "epoch: 2 loss: 0.26173096778137345\n",
      "epoch: 2 loss: 0.2615263026828567\n",
      "epoch: 2 loss: 0.26114144109793613\n",
      "epoch: 2 loss: 0.26029114427143024\n",
      "epoch: 2 loss: 0.2602753886427635\n",
      "epoch: 2 loss: 0.2595387624017894\n",
      "epoch: 2 loss: 0.25932716952591406\n",
      "epoch: 2 loss: 0.25834605140345435\n",
      "epoch: 2 loss: 0.25750793703766756\n",
      "epoch: 2 loss: 0.2569740732115778\n",
      "epoch: 2 loss: 0.2562594680984815\n",
      "epoch: 2 loss: 0.2559559395455796\n",
      "epoch: 2 loss: 0.2555177075273179\n",
      "epoch: 2 loss: 0.25496825684482854\n",
      "epoch: 2 loss: 0.2546556391582197\n",
      "epoch: 2 loss: 0.2539828732460737\n",
      "epoch: 2 loss: 0.25357624356653174\n",
      "epoch: 2 loss: 0.2530727130432542\n",
      "epoch: 2 loss: 0.2526774268527076\n",
      "epoch: 2 loss: 0.2522663186683699\n",
      "epoch: 2 loss: 0.25178201674060385\n",
      "epoch: 2 loss: 0.25148073422855566\n",
      "epoch: 2 loss: 0.2510377329431082\n",
      "epoch: 2 loss: 0.2505705337714532\n",
      "epoch: 2 loss: 0.2501007465992944\n",
      "epoch: 2 loss: 0.24957640318820873\n",
      "epoch: 2 loss: 0.24924279806799576\n",
      "epoch: 2 loss: 0.2486853271602623\n",
      "epoch: 2 loss: 0.24832795869026866\n",
      "epoch: 2 loss: 0.2479976141010411\n",
      "epoch: 2 loss: 0.24767670240539771\n",
      "epoch: 2 loss: 0.24718536021131457\n",
      "epoch: 2 loss: 0.24680417091099183\n",
      "epoch: 2 loss: 0.24649747767869165\n",
      "epoch: 2 loss: 0.2461268010670724\n",
      "epoch: 2 loss: 0.2458117422035762\n",
      "epoch: 2 loss: 0.24543281850260748\n",
      "epoch: 2 loss: 0.24510934383918842\n",
      "epoch: 2 loss: 0.24482422339385504\n",
      "epoch: 2 loss: 0.24452715628855937\n",
      "epoch: 2 loss: 0.24435759279131888\n",
      "epoch: 2 loss: 0.2441064216196537\n",
      "epoch: 2 loss: 0.24398902653680218\n",
      "epoch: 2 loss: 0.24360266874233882\n",
      "epoch: 2 loss: 0.24319231464893004\n",
      "epoch: 2 loss: 0.24283643756993115\n",
      "epoch: 2 loss: 0.24239068890427365\n",
      "epoch: 2 loss: 0.24203678919229565\n",
      "epoch: 2 loss: 0.2418136443449072\n",
      "epoch: 2 loss: 0.2414766711227241\n",
      "epoch: 2 loss: 0.24114990201066522\n",
      "epoch: 2 loss: 0.240809987303476\n",
      "epoch: 2 loss: 0.24066021004798768\n",
      "epoch: 2 loss: 0.24042102435434407\n",
      "epoch: 2 loss: 0.2401556766937288\n",
      "epoch: 2 loss: 0.23981808598670695\n",
      "epoch: 2 loss: 0.23960870141675183\n",
      "epoch: 2 loss: 0.23924806900322437\n",
      "epoch: 2 loss: 0.23879398668325075\n",
      "epoch: 2 loss: 0.23850413400759088\n",
      "epoch: 2 loss: 0.23819622462517337\n",
      "epoch: 2 loss: 0.23793064984492956\n",
      "epoch: 2 loss: 0.2376588030574248\n",
      "epoch: 2 loss: 0.23738374485045063\n",
      "epoch: 2 loss: 0.2372351024864298\n",
      "epoch: 2 loss: 0.23689154817909003\n",
      "epoch: 2 loss: 0.23657595079281543\n",
      "epoch: 2 loss: 0.2361515686719441\n",
      "epoch: 2 loss: 0.23581714196835907\n",
      "epoch: 2 loss: 0.23564694433544692\n",
      "epoch: 2 loss: 0.23538120669978005\n",
      "epoch: 2 loss: 0.23512056557075034\n",
      "epoch: 2 loss: 0.2348324337921967\n",
      "epoch: 2 loss: 0.2345165008885993\n",
      "epoch: 2 loss: 0.23437435730881648\n",
      "epoch: 2 loss: 0.23401463648135012\n",
      "epoch: 2 loss: 0.2337425580835557\n",
      "epoch: 2 loss: 0.23340442438077713\n",
      "epoch: 2 loss: 0.23318182166039417\n",
      "epoch: 2 loss: 0.23288760632276534\n",
      "epoch: 2 loss: 0.2326459040330804\n",
      "epoch: 2 loss: 0.23237045689261165\n",
      "epoch: 2 loss: 0.23204774159269456\n",
      "epoch: 2 loss: 0.23184366113806176\n",
      "epoch: 2 loss: 0.23164733876200283\n",
      "epoch: 2 loss: 0.23144227333987752\n",
      "epoch: 2 loss: 0.23107557325077452\n",
      "epoch: 2 loss: 0.23070643798249668\n",
      "epoch: 2 loss: 0.23047140273984854\n",
      "epoch: 2 loss: 0.23019070821784196\n",
      "epoch: 2 loss: 0.22987899872660636\n",
      "epoch: 2 loss: 0.22960590358291352\n",
      "epoch: 2 loss: 0.22932802216978523\n",
      "epoch: 2 loss: 0.22914152391022072\n",
      "epoch: 2 loss: 0.22885067539737206\n",
      "epoch: 2 loss: 0.22874252045956942\n",
      "epoch: 2 loss: 0.2285255186657869\n",
      "epoch: 2 loss: 0.22830741738505436\n",
      "epoch: 2 loss: 0.22805415042921118\n",
      "epoch: 2 loss: 0.22785492086254838\n",
      "epoch: 2 loss: 0.22763074579062284\n",
      "epoch: 2 loss: 0.22739308146102463\n",
      "epoch: 2 loss: 0.22712106510561747\n",
      "epoch: 2 loss: 0.2269375317554543\n",
      "epoch: 2 loss: 0.22664457064309565\n",
      "epoch: 2 loss: 0.22636816679899183\n",
      "epoch: 2 loss: 0.22610136441727902\n",
      "epoch: 2 loss: 0.22584398758243507\n",
      "epoch: 2 loss: 0.22557254314318403\n",
      "epoch: 2 loss: 0.22534294917972553\n",
      "epoch: 2 loss: 0.2250638778096643\n",
      "epoch: 2 loss: 0.22482134351787503\n",
      "epoch: 2 loss: 0.22452229110359334\n",
      "epoch: 2 loss: 0.22421831805762407\n",
      "epoch: 2 loss: 0.22407342595041999\n",
      "epoch: 2 loss: 0.2238883693019549\n",
      "epoch: 2 loss: 0.22373113998810187\n",
      "epoch: 2 loss: 0.2235314415620738\n",
      "epoch: 2 loss: 0.22330560287813736\n",
      "epoch: 2 loss: 0.2231007178156794\n",
      "epoch: 2 loss: 0.22288486258156837\n",
      "epoch: 2 loss: 0.2226431932921211\n",
      "epoch: 2 loss: 0.22233885009387497\n",
      "epoch: 2 loss: 0.22217358318216437\n",
      "epoch: 2 loss: 0.22199703599765616\n",
      "epoch: 2 loss: 0.22170860387384891\n",
      "epoch: 2 loss: 0.22151407003865478\n",
      "epoch: 2 loss: 0.22134951127256142\n",
      "epoch: 2 loss: 0.2212034042901788\n",
      "epoch: 2 loss: 0.22103696297854186\n",
      "epoch: 2 loss: 0.22087641003456981\n",
      "epoch: 2 loss: 0.22062318163912698\n",
      "epoch: 2 loss: 0.22035027240921637\n",
      "epoch: 2 loss: 0.22015583760415514\n",
      "epoch: 2 loss: 0.21998677856823395\n",
      "epoch: 2 loss: 0.21977847157594035\n",
      "epoch: 2 loss: 0.21961922204633902\n",
      "epoch: 2 loss: 0.21940797178377938\n",
      "epoch: 2 loss: 0.21916070126987605\n",
      "epoch: 2 loss: 0.21891828174221103\n",
      "epoch: 2 loss: 0.21869814464024134\n",
      "epoch: 2 loss: 0.21850510726005518\n",
      "epoch: 2 loss: 0.21833834149322268\n",
      "epoch: 2 loss: 0.218111543600144\n",
      "epoch: 2 loss: 0.2178996307015752\n",
      "epoch: 2 loss: 0.2176624693431788\n",
      "epoch: 2 loss: 0.21743915678717154\n",
      "epoch: 2 loss: 0.2172132322339566\n",
      "epoch: 2 loss: 0.216985412283999\n",
      "epoch: 2 loss: 0.21675882649243525\n",
      "epoch: 2 loss: 0.21654865068761078\n",
      "epoch: 2 loss: 0.21634311788184668\n",
      "epoch: 2 loss: 0.2160896846516566\n",
      "epoch: 2 loss: 0.21592095096773925\n",
      "epoch: 2 loss: 0.21569893182348954\n",
      "epoch: 2 loss: 0.21553593139115132\n",
      "epoch: 2 loss: 0.2153569642044799\n",
      "epoch: 2 loss: 0.21528761507809427\n",
      "epoch: 3 loss: 0.20277687311172485\n",
      "epoch: 3 loss: 0.20218158178031445\n",
      "epoch: 3 loss: 0.20039545521140098\n",
      "epoch: 3 loss: 0.2019920190796256\n",
      "epoch: 3 loss: 0.2045029406249523\n",
      "epoch: 3 loss: 0.20345643820861975\n",
      "epoch: 3 loss: 0.20107818362968308\n",
      "epoch: 3 loss: 0.19957140469923615\n",
      "epoch: 3 loss: 0.1984331275853846\n",
      "epoch: 3 loss: 0.19790238611400127\n",
      "epoch: 3 loss: 0.19818258102644573\n",
      "epoch: 3 loss: 0.19780873072644076\n",
      "epoch: 3 loss: 0.1968328435260516\n",
      "epoch: 3 loss: 0.19607684846435275\n",
      "epoch: 3 loss: 0.1961454150577386\n",
      "epoch: 3 loss: 0.19500226587988437\n",
      "epoch: 3 loss: 0.1943268505965962\n",
      "epoch: 3 loss: 0.1930807005200121\n",
      "epoch: 3 loss: 0.19236069396138192\n",
      "epoch: 3 loss: 0.1915651699528098\n",
      "epoch: 3 loss: 0.191226695087694\n",
      "epoch: 3 loss: 0.19114042193713512\n",
      "epoch: 3 loss: 0.19097710373285023\n",
      "epoch: 3 loss: 0.18979975166730584\n",
      "epoch: 3 loss: 0.18915440015494822\n",
      "epoch: 3 loss: 0.18888505320422924\n",
      "epoch: 3 loss: 0.18849232871499327\n",
      "epoch: 3 loss: 0.18846223869227938\n",
      "epoch: 3 loss: 0.18790732850802355\n",
      "epoch: 3 loss: 0.18736373230814934\n",
      "epoch: 3 loss: 0.18705872181923158\n",
      "epoch: 3 loss: 0.18655655847396702\n",
      "epoch: 3 loss: 0.18602950421698167\n",
      "epoch: 3 loss: 0.18564701847293796\n",
      "epoch: 3 loss: 0.18518188993845666\n",
      "epoch: 3 loss: 0.18454214696668916\n",
      "epoch: 3 loss: 0.18430376761668438\n",
      "epoch: 3 loss: 0.1839852060926588\n",
      "epoch: 3 loss: 0.18375121037929487\n",
      "epoch: 3 loss: 0.1835877406038344\n",
      "epoch: 3 loss: 0.18351688385009765\n",
      "epoch: 3 loss: 0.18297344703404678\n",
      "epoch: 3 loss: 0.1825076053308886\n",
      "epoch: 3 loss: 0.18222955451770262\n",
      "epoch: 3 loss: 0.18184523939258523\n",
      "epoch: 3 loss: 0.18148731190063383\n",
      "epoch: 3 loss: 0.18081252978361667\n",
      "epoch: 3 loss: 0.18052085454886158\n",
      "epoch: 3 loss: 0.18044813834891027\n",
      "epoch: 3 loss: 0.18041596226394177\n",
      "epoch: 3 loss: 0.1801643163988403\n",
      "epoch: 3 loss: 0.17990110295179945\n",
      "epoch: 3 loss: 0.17988168683676226\n",
      "epoch: 3 loss: 0.17946391803108983\n",
      "epoch: 3 loss: 0.1791835178096186\n",
      "epoch: 3 loss: 0.17891785338121866\n",
      "epoch: 3 loss: 0.1786254427477456\n",
      "epoch: 3 loss: 0.17831353625475332\n",
      "epoch: 3 loss: 0.17810004890849024\n",
      "epoch: 3 loss: 0.1779164208720128\n",
      "epoch: 3 loss: 0.17771642347950428\n",
      "epoch: 3 loss: 0.17748936599180584\n",
      "epoch: 3 loss: 0.17740555875712916\n",
      "epoch: 3 loss: 0.17729713448206894\n",
      "epoch: 3 loss: 0.17725893130096104\n"
     ]
    }
   ],
   "source": [
    "graph0 = tf.Graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "import os\n",
    "\n",
    "with tf.Session(target='', graph=graph0, config=config) as sess:\n",
    "    record_path = '/barn2/yuan/datasets/300W_LP/flat_dataset.record'\n",
    "    dataseto = load_dataset.Dataset()\n",
    "    dataset = dataseto.get_tf_dataset(record_path, batch_size=16)\n",
    "    dataset = dataset.prefetch(buffer_size=200)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_example = iterator.get_next()\n",
    "    image_tensor, heatmap_groundtruth_tensor = next_example\n",
    "\n",
    "    print(image_tensor)\n",
    "    print(heatmap_groundtruth_tensor)\n",
    "    \n",
    "    heatmap_inferred_tensors = model.fan(x=image_tensor, num_modules=4)\n",
    "    print('inferred length: ', len(heatmap_inferred_tensors))\n",
    "    loss_op = 0.0\n",
    "    for heatmap_inferred_tensor in heatmap_inferred_tensors:\n",
    "        loss_op = loss_op + tf.losses.mean_squared_error(labels=heatmap_groundtruth_tensor, predictions=heatmap_inferred_tensor)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss_op)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    os.makedirs('./saved_model/face_landmark/20180921_01', exist_ok=True)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_epoch = 200\n",
    "    for i_epoch in range(n_epoch):\n",
    "        sess.run(iterator.initializer)\n",
    "        loss = 0.0\n",
    "        n_steps = 0\n",
    "        step_cnt = 0\n",
    "        while True:\n",
    "            step_cnt += 1\n",
    "            try:\n",
    "                loss_i, _ = sess.run([loss_op, train_op])\n",
    "                loss += loss_i\n",
    "                n_steps += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                saver.save(sess, './saved_model/face_landmark/20180921_01')\n",
    "                break\n",
    "            if step_cnt % 20 == 0:\n",
    "                print('epoch:', i_epoch + 1, 'loss:', loss / n_steps)\n",
    "        print('epoch:', i_epoch + 1, 'loss:', loss / n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versions {\n",
      "  producer: 27\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "print(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
